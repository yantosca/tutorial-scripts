Dry-run in GC Classic 13.2.1 and later
Bob Yantosca, GCST, Aug 2021

* What's new?

** New GEOS-Chem data servers (aka "mirrors") are online:

1. U. Rochester

Contains the GCAP met field data.  If you are interested in running
historical or future simulations with GEOS-Chem, then this is the data
server to use.  (Maintained by Lee Murray's group at U. Rochester.)

2. WashU @ St. Louis

Contains all GEOS-Chem met field & emissions data.  Will eventually
replace the ComputeCanada server.  (Maintained by Randall Martin's
group at Washington University in St. Louis.)

** Configurable settings:

The Python script that downloads data (download_data.py) now reads
configurable settings, such as information about data mirrors and
default restart files, from a YAML file.  We'll take a look at this
later.

This enables these settings to be changed if necessary without having
to hardwire them into the Python code.


* Download the GEOS-Chem Classic source code

NOTE: I am on an AWS EC2 cloud instance, because it is easier for me
to start from an empty ExtData folder.  But you can follow my steps
regardless of what type of computer system you are on.

** Clone the GCClassic wrapper from Github:

#+BEGIN_SRC bash
  git clone https://github.com/geoschem/GCClassic.git
#+END_SRC

** Navigate to the GCClassic folder and initialize the submodules:

#+BEGIN_SRC bash
  cd GCClassic
  git submodule update --init --recursive
#+END_SRC

** For more information:

To learn more about how to get set up with GEOS-Chem 13.0.0, please
view our video tutorials at https://youtube.com/c/geoschem


* EXTRA STEP JUST FOR THIS VIDEO!

NOTE: Because GEOS-Chem 13.2.1 is not yet released at this time (early
August 2021), I need to check out the branch of the "science codebase"
where the 13.2.1 updates are kept.

#+BEGIN_SRC bash
  git -C src/GEOS-Chem checkout patch/13.2.1
#+END_SRC

But note: once 13.2.1 is released, these updates will already be in
the main branch, and you won't have to do this extra step.


* Create a run directory for a fullchem "standard" simulation

I'll navigate to the GCClassic/run folder and execute the "createRunDir.sh"
script.  This will create a new GEOS-Chem Classic run directory from
scratch.

#+BEGIN_SRC bash
  cd run
  ./createRunDir.sh
#+END_SRC

I'll follow the prompts to create a run directory (with MERRA-2 met fields,
4 x 5 grid, and all 72 levels).  I'll create the GEOS-Chem run
directory in my ~/dry-run-video/ folder.


* Edit run-directory configuration files

I'll navigate to the run directory that I just created:

#+BEGIN_SRC bash
  cd /home/ubuntu/dry-run-video/gc_merra2_fullchem
  ls
#+END_SRC

For the purposes of this video, I'll configure GEOS-Chem Classic to
run for a short simulation (say 1 hour).  We'll need to edit the
following configuration files:

** input.geos

Change end date to 20190701 010000

** HISTORY.rc

Change frequency & duration from 00000100 000000 to 00000000 010000

** HEMCO_Config.rc

Change "Monthly" to "End"


* View download_data.yml file

I'd like to take a look at the download_data.yml file.

NOTE: The YAML file is mostly meant for easy updating by the GEOS-Chem
Support Team.  GEOS-Chem users shouldn't have to edit this file.

** mirrors

1. Contains metadata for the different data servers (aka mirrors).
   We note if the server is an Amazon S3 bucket or not.

2. You can use refer to a mirror by its name or short name.

3. The command will be used to download data from the mirror.


** restarts

This section lists the default restart files that will be placed
into a GEOS-Chem classic run directory.

The restart file paths and file names used to be hard-coded into the
download_data.py script, but these are now kept in the
download_data.yml file.

The default restart files are usually updated at each GEOS-Chem major
version (i.e. 12.0.0, 13.0.0, and when it happens, 14.0.0) and are
taken from the relevant 1-year or 10-year benchmark simulations.


* Configure and build GEOS-Chem

Now I'll configure GEOS-Chem with CMake:

#+BEGIN_SRC bash
  cd build
  cmake ../CodeDir -DRUNDIR=..
#+END_SRC

and then build the GEOS-Chem executable:

#+BEGIN_SRC bash
  make -j
  make -j install
#+END_SRC

I'll return to the run directory.

#+BEGIN_SRC bash
  cd ..
  ls
#+END_SRC

and we can see the "gcclassic" executable file here.


* Perform the dry-run simulation

I'll now run GEOS-Chem in dry-run mode:

#+BEGIN_SRC bash
  ./gcclassic --dryrun > log.dryrun
#+END_SRC

This will produce a file named "log.dryrun" (you can name it anything
you want, but I like "log.dryrun").

The log.dryrun file contains a list of all the files that GEOS-Chem
will try to read.  Each file will be marked with either "Opening"
(which means that the file is found on disk, or "REQUIRED FILE NOT
FOUND", which means that the file has yet to be downloaded.


* Download data with the python script (download_data.py)

Now that I've generated the dryrun log file (i.e log.dryrun),
(log.dryrun), I can start the data download process.

** General usage

To download data from a particular mirror, type:

#+BEGIN_SRC emacs-lisp
./download_data.py LOG MIRROR-NAME
#+END_SRC

This will download the data files to local ExtData folder.

LOG is the name of the dry-run log file (e.g. "log.dryrun").

MIRROR-NAME may be either the long name (e.g. "amazon",
"computecanada", etc.) or short name ("aws", "cc", etc) of the data
mirror, as listed in the download_data.yml file.

The download_data.py script will also generate the list of unique
data files needed for the simulation.  This can be useful for
documentation purposes.  The log of unique data files will be the same
as your dryrun log file, with the suffix ".unique."

** To skip downloading data

If you wish to only generate the list of unique data files without
downloading data, you can type:

#+BEGIN_SRC emacs-lisp
./download_data.py LOG MIRROR-NAME skip
#+END_SRC


* Example: download from the Amazon S3 bucket

IMPORTANT NOTE! Only download data from the GEOS-Chem S3 bucket
(s3://gcgrid) if you are on an AWS EC2 cloud instance!  It is free to
download from Amazon S3 to Amazon EC2, but if you download data out of
Amazon, you will incur egress fees.

But since I am are already on the AWS cloud, I can proceed to download
data from the Amazon S3 bucket (s3://gcgrid):

#+BEGIN_SRC emacs-lisp
time -p ./download_data.py log.dryrun amazon
#+END_SRC

The download process should take a few minutes.

The time -p command isn't strictly necessary, but that will show us
how long the download process takes.  It should take a few minutes.


* Run GEOS-Chem

Now that the dry-run has completed, I will try to run GEOS-Chem.

#+BEGIN_SRC bash
./gcclassic | tee GC.log
#+END_SRC

The "tee" command will send output to a log file and to the screen, so
that we can see the run progress in real time.


* If there are still missing files

Depending on how often the data mirrors are synced, it is possible
that some mirrors might not yet have obtained the most recent
GEOS-Chem data files.  If this happens, then here is what to do:

** Do another dry-run simulation

If the GEOS-Chem simulation dies because a file is missing, then the
best thing to do is to run GEOS-Chem again in dry-run mode.  This will
find only the files that need to be downloaded, and skip any files
that were downloaded in previous dry-runs.

I'll start a new dry-run here and send the output to a diffrent log file.

#+BEGIN_SRC bash
  ./gcclassic --dryrun > log.dryrun.2
#+END_SRC

** Then download data from a different mirror

Now I can attempt to download the missing files from a different
mirror, such as WashU:

#+BEGIN_SRC bash
./download_data.py log.dryrun.2 washu
#+END_SRC

This should pick up only the files that were listed as missing in
log.dryrun.2.


* Run GEOS-Chem again

I'll try to run GEOS-Chem again now, that all of the missing data
files have been downloaded.

#+BEGIN_SRC bash
./gcclassic
#+END_SRC

And there we have it!  A bootstrapped GEOS-Chem Classic simulation
from an empty ExtData folder.
